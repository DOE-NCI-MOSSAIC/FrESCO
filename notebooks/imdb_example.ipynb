{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7424773b",
   "metadata": {},
   "source": [
    "This is an example for running the FrESCO library with the imdb benchmark data. Included within the FrESCO repository is a text-only version of the imdb dataset and a script for processing data prior to training. The `data_stup.py` script may be used to process data into the expected format for the FrESCO model training codebase.  If you've not already done so, go to the data directory and unzip the dataset using the command `$ tar -xf imdb.tar.gz`, then start formatting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd80a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fd8f0",
   "metadata": {},
   "source": [
    "We'll need a helper function to tokenize the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84dd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(d, model):\n",
    "    ints = [model.wv.key_to_index.get(d.lower()) for d in d.split(' ')]\n",
    "    unk = len(model.wv.key_to_index)\n",
    "    return [x if x is not None else unk for x in ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e014d",
   "metadata": {},
   "source": [
    "Now we'll set the random number seed, dimension of the word embeddings, and size of the train, test, and val splits. Then we'll read in the raw text into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62627840",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "train_split = 0.75\n",
    "val_split = 0.15\n",
    "test_split = 0.10\n",
    "\n",
    "embed_dim = 300\n",
    "\n",
    "df = pd.read_csv('../data/imdb/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8356c",
   "metadata": {},
   "source": [
    "Next we'll do some initial cleaning of the data, stripping out non-alphanumeric and escape characters and words shorter than 2 characters. Lastly, all the remaining words in the reviews will be split into individual strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff793b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d.lower() for d in df['review']]\n",
    "data = [gensim.parsing.preprocessing.strip_tags(d) for d in data]\n",
    "data = [gensim.parsing.preprocessing.strip_non_alphanum(d) for d in data]\n",
    "data = [gensim.parsing.preprocessing.strip_short(d, minsize=2) for d in data]\n",
    "data = [d.split(' ') for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc4289",
   "metadata": {},
   "source": [
    "Now we can split the cleaned data into the train/test/val proportions we specified above and start putting them into a dataframe for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aed965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_tmp, y_train, y_tmp = train_test_split(df['review'], df['sentiment'], test_size=1-train_split,\n",
    "                                                  random_state=seed)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_tmp, y_tmp, test_size=test_split/(test_split + val_split),\n",
    "                                                random_state=seed)\n",
    "\n",
    "train_df = pd.DataFrame(x_train, columns=['review', 'X', 'sentiment', 'split'])\n",
    "val_df = pd.DataFrame(x_val, columns=['review', 'X', 'sentiment', 'split'])\n",
    "test_df = pd.DataFrame(x_test, columns=['review', 'X', 'sentiment', 'split'])\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "val_df['split'] = 'val'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "train_df['sentiment'] = y_train\n",
    "val_df['sentiment'] = y_val\n",
    "test_df['sentiment'] = y_test\n",
    "\n",
    "train_data = pd.concat([train_df, val_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252e84c",
   "metadata": {},
   "source": [
    "The next step in the process is to build the vocab from the train and val sets, then train the word embeddings on that vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91481832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab and word embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198994029, 260081200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating vocab and word embeddings\")\n",
    "model = gensim.models.word2vec.Word2Vec(vector_size=embed_dim, min_count=2, epochs=25, workers=4)\n",
    "model.build_vocab(train_data['review'].str.split())\n",
    "model.train(train_data['review'].str.split(), total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861a02a",
   "metadata": {},
   "source": [
    "We next need to tokenize all of the reports and save them in the `X` field of the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6cc636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37500/37500 [00:01<00:00, 23348.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:00<00:00, 23383.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 23552.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['X'] = train_df['review'].progress_apply(lambda d: word2int(d, model))\n",
    "val_df['X'] = val_df['review'].progress_apply(lambda d: word2int(d, model))\n",
    "test_df['X'] = test_df['review'].progress_apply(lambda d: word2int(d, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f2b47",
   "metadata": {},
   "source": [
    "Since the global vocab is based on the train and val sets, there will be some words in the test set not appearing in the vocab. To handle this situation, we map those words to the `<unk>`, unknown, token and add a random embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7190711",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = [model.wv.vectors[index] for index in model.wv.key_to_index.values()]\n",
    "rng = np.random.default_rng(seed)\n",
    "unk_embed = rng.normal(size=(1, embed_dim), scale=0.1)\n",
    "w2v = np.append(word_vecs, unk_embed, axis=0)\n",
    "\n",
    "id2word = {v: k for k, v in model.wv.key_to_index.items()}\n",
    "id2word[len(model.wv.key_to_index)] = \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901a16b",
   "metadata": {},
   "source": [
    "The last step prior to training a model is to save the needed files in a convenient location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5451eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.concat([train_df, val_df, test_df])\n",
    "df_out.to_csv(\"../data/imdb/data_fold0.csv\", index=False)\n",
    "\n",
    "labels = set(df['sentiment'])\n",
    "id2label = {'sentiment': {i: l for i, l in enumerate(labels)}}\n",
    "with open('../data/imdb/id2labels_fold0.json', 'w') as f:\n",
    "    json.dump(id2label, f)\n",
    "\n",
    "with open('../data/imdb/id2word.json', 'w') as f:\n",
    "    json.dump(id2word, f)\n",
    "np.save('../data/imdb/word_embeds_fold0.npy', w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460646d6",
   "metadata": {},
   "source": [
    "Now we're ready to train a model. Let's get the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c2a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fresco\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa26c90",
   "metadata": {},
   "source": [
    "The FrESCO library is typically run from the command line with arguments specifying the model type and model args, so we'll have to set them up manually for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5913468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    _ = parser.add_argument(\"--model\", \"-m\", type=str, default='ie',\n",
    "                        help=\"\"\"which type of model to create. Must be either\n",
    "                                IE (information extraction) or clc (case-level context).\"\"\")\n",
    "    _ = parser.add_argument('--model_path', '-mp', type=str, default='',\n",
    "                       help=\"\"\"this is the location of the model\n",
    "                               that will used to make predictions\"\"\")\n",
    "    _ = parser.add_argument('--data_path', '-dp', type=str, default='',\n",
    "                        help=\"\"\"where the data will load from. The default is\n",
    "                                the path saved in the model\"\"\")\n",
    "    _ = parser.add_argument('--model_args', '-args', type=str, default='',\n",
    "                        help=\"\"\"file specifying the model or clc args; default is in\n",
    "                                the fresco directory\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7fe4b",
   "metadata": {},
   "source": [
    "We are going to train a multi-task classification model on the P3B3 dataset, so we'll specify an `information extraction` model and point to the P3B3 model args file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e7eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['-m', 'ie', '-args', '../configs/imdb_args.yml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1504aa2",
   "metadata": {},
   "source": [
    "With these arguments specified, just need a few imports before we're ready to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91cc5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fresco import run_ie\n",
    "\n",
    "from fresco.validate import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a9fb57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating kwargs in model_args.yml file\n",
      "Loading data and creating DataLoaders\n",
      "Loading data from ../data/imdb/\n",
      "Num workers: 4, reproducible: True\n",
      "Training on 37500 validate on 7500\n",
      "\n",
      "Defining a model\n",
      "Creating model trainer\n",
      "Training a mthisan model with 2 cuda device\n",
      "\n",
      "\n",
      "epoch: 1\n",
      "\n",
      "training time 40.18\n",
      "Training loss: 0.467834\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.7522,     0.7522\n",
      "\n",
      "epoch 1 validation\n",
      "\n",
      "epoch 1 val loss: 0.32461786, best val loss: inf\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8588,     0.8580\n",
      "\n",
      "epoch: 2\n",
      "\n",
      "training time 40.90\n",
      "Training loss: 0.374819\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8425,     0.8425\n",
      "\n",
      "epoch 2 validation\n",
      "\n",
      "epoch 2 val loss: 0.29770253, best val loss: 0.32461786\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8725,     0.8725\n",
      "\n",
      "epoch: 3\n",
      "\n",
      "training time 40.94\n",
      "Training loss: 0.365319\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8571,     0.8571\n",
      "\n",
      "epoch 3 validation\n",
      "\n",
      "epoch 3 val loss: 0.28573914, best val loss: 0.29770253\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8824,     0.8823\n",
      "\n",
      "epoch: 4\n",
      "\n",
      "training time 40.95\n",
      "Training loss: 0.368638\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8661,     0.8661\n",
      "\n",
      "epoch 4 validation\n",
      "\n",
      "epoch 4 val loss: 0.28191535, best val loss: 0.28573914\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8864,     0.8863\n",
      "\n",
      "epoch: 5\n",
      "\n",
      "training time 41.10\n",
      "Training loss: 0.294115\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8713,     0.8713\n",
      "\n",
      "epoch 5 validation\n",
      "\n",
      "epoch 5 val loss: 0.27684327, best val loss: 0.28191535\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8899,     0.8897\n",
      "\n",
      "epoch: 6\n",
      "\n",
      "training time 41.02\n",
      "Training loss: 0.237020\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8773,     0.8773\n",
      "\n",
      "epoch 6 validation\n",
      "\n",
      "epoch 6 val loss: 0.27373572, best val loss: 0.27684327\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8917,     0.8917\n",
      "\n",
      "epoch: 7\n",
      "\n",
      "training time 40.99\n",
      "Training loss: 0.261353\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8819,     0.8819\n",
      "\n",
      "epoch 7 validation\n",
      "\n",
      "epoch 7 val loss: 0.27793957, best val loss: 0.27373572\n",
      "patience counter is at 1 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8921,     0.8918\n",
      "\n",
      "epoch: 8\n",
      "\n",
      "training time 41.16\n",
      "Training loss: 0.314205\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8845,     0.8845\n",
      "\n",
      "epoch 8 validation\n",
      "\n",
      "epoch 8 val loss: 0.27016858, best val loss: 0.27373572\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8945,     0.8943\n",
      "\n",
      "epoch: 9\n",
      "\n",
      "training time 41.01\n",
      "Training loss: 0.226156\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8885,     0.8885\n",
      "\n",
      "epoch 9 validation\n",
      "\n",
      "epoch 9 val loss: 0.27105088, best val loss: 0.27016858\n",
      "patience counter is at 1 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8937,     0.8934\n",
      "\n",
      "epoch: 10\n",
      "\n",
      "training time 40.96\n",
      "Training loss: 0.302527\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8905,     0.8905\n",
      "\n",
      "epoch 10 validation\n",
      "\n",
      "epoch 10 val loss: 0.26275557, best val loss: 0.27016858\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8965,     0.8964\n",
      "\n",
      "epoch: 11\n",
      "\n",
      "training time 41.02\n",
      "Training loss: 0.321275\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8937,     0.8937\n",
      "\n",
      "epoch 11 validation\n",
      "\n",
      "epoch 11 val loss: 0.27049048, best val loss: 0.26275557\n",
      "patience counter is at 1 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8980,     0.8976\n",
      "\n",
      "epoch: 12\n",
      "\n",
      "training time 41.04\n",
      "Training loss: 0.156769\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8959,     0.8959\n",
      "\n",
      "epoch 12 validation\n",
      "\n",
      "epoch 12 val loss: 0.27671595, best val loss: 0.26275557\n",
      "patience counter is at 2 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8963,     0.8958\n",
      "\n",
      "epoch: 13\n",
      "\n",
      "training time 40.95\n",
      "Training loss: 0.222866\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8991,     0.8991\n",
      "\n",
      "epoch 13 validation\n",
      "\n",
      "epoch 13 val loss: 0.26249462, best val loss: 0.26275557\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9028,     0.9026\n",
      "\n",
      "epoch: 14\n",
      "\n",
      "training time 40.95\n",
      "Training loss: 0.181751\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9009,     0.9009\n",
      "\n",
      "epoch 14 validation\n",
      "\n",
      "epoch 14 val loss: 0.26018575, best val loss: 0.26249462\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9025,     0.9025\n",
      "\n",
      "epoch: 15\n",
      "\n",
      "training time 41.12\n",
      "Training loss: 0.211659\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9039,     0.9039\n",
      "\n",
      "epoch 15 validation\n",
      "\n",
      "epoch 15 val loss: 0.27947039, best val loss: 0.26018575\n",
      "patience counter is at 1 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.8957,     0.8951\n",
      "\n",
      "epoch: 16\n",
      "\n",
      "training time 41.07\n",
      "Training loss: 0.213826\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9045,     0.9045\n",
      "\n",
      "epoch 16 validation\n",
      "\n",
      "epoch 16 val loss: 0.25642987, best val loss: 0.26018575\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9037,     0.9036\n",
      "\n",
      "epoch: 17\n",
      "\n",
      "training time 40.95\n",
      "Training loss: 0.219663\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9072,     0.9072\n",
      "\n",
      "epoch 17 validation\n",
      "\n",
      "epoch 17 val loss: 0.25579571, best val loss: 0.25642987\n",
      "patience counter is at 0 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9045,     0.9045\n",
      "\n",
      "epoch: 18\n",
      "\n",
      "training time 41.06\n",
      "Training loss: 0.198756\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9099,     0.9099\n",
      "\n",
      "epoch 18 validation\n",
      "\n",
      "epoch 18 val loss: 0.26291793, best val loss: 0.25579571\n",
      "patience counter is at 1 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9069,     0.9067\n",
      "\n",
      "epoch: 19\n",
      "\n",
      "training time 41.03\n",
      "Training loss: 0.275782\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9131,     0.9131\n",
      "\n",
      "epoch 19 validation\n",
      "\n",
      "epoch 19 val loss: 0.25687561, best val loss: 0.25579571\n",
      "patience counter is at 2 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9072,     0.9071\n",
      "\n",
      "epoch: 20\n",
      "\n",
      "training time 40.93\n",
      "Training loss: 0.222284\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9133,     0.9133\n",
      "\n",
      "epoch 20 validation\n",
      "\n",
      "epoch 20 val loss: 0.26461052, best val loss: 0.25579571\n",
      "patience counter is at 3 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9055,     0.9052\n",
      "\n",
      "epoch: 21\n",
      "\n",
      "training time 41.09\n",
      "Training loss: 0.183604\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9179,     0.9179\n",
      "\n",
      "epoch 21 validation\n",
      "\n",
      "epoch 21 val loss: 0.26141117, best val loss: 0.25579571\n",
      "patience counter is at 4 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9085,     0.9084\n",
      "\n",
      "epoch: 22\n",
      "\n",
      "training time 40.95\n",
      "Training loss: 0.173037\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9185,     0.9185\n",
      "\n",
      "epoch 22 validation\n",
      "\n",
      "epoch 22 val loss: 0.26103199, best val loss: 0.25579571\n",
      "patience counter is at 5 of 5\n",
      "        task:      micro        macro\n",
      "   sentiment:     0.9104,     0.9104\n",
      "saving to savedmodels/imdb_model/imdb_model_fold0.h5\n",
      "\n",
      "Scoring test set\n",
      "\n",
      "Predicting train set\n",
      "\n",
      "Predicting val set\n",
      "\n",
      "Predicting test set\n",
      "Saving predictions to csv\n",
      "\n",
      "Evaluating train set\n",
      "\n",
      "Evaluating val set\n",
      "\n",
      "Evaluating test set\n",
      "Saving predictions to csv\n",
      "\n",
      "Full model file has been saved at savedmodels/imdb_model/imdb_model_20230523135300_fold0.h5\n"
     ]
    }
   ],
   "source": [
    "run_ie.run_ie(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637777d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

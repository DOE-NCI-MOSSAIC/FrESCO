%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Spannaus, Adam at 2022-12-16 11:01:10 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{dac,
	abstract = {We introduce a novel method to combat label noise when training deep neural networks for classification. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while continuing to learn and improve classification performance on the non-abstained samples. We show how such a deep abstaining classifier (DAC) can be used for robust learning in the presence of different types of label noise. In the case of structured or systematic label noise {--} where noisy training labels or confusing examples are correlated with underlying features of the data{--} training with abstention enables representation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samples that are likely to have label noise. We provide analytical results on the loss function behavior that enable dynamic adaption of abstention rates based on learning progress during training. We demonstrate the utility of the deep abstaining classifier for various image classification tasks under different types of label noise; in the case of arbitrary label noise, we show significant im- provements over previously published results on multiple image benchmarks.},
	author = {Thulasidasan, Sunil and Bhattacharya, Tanmoy and Bilmes, Jeff and Chennupati, Gopinath and Mohd-Yusof, Jamal},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	date-added = {2022-12-15 15:45:40 -0500},
	date-modified = {2022-12-15 15:45:49 -0500},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = {09--15 Jun},
	pages = {6234--6243},
	pdf = {http://proceedings.mlr.press/v97/thulasidasan19a/thulasidasan19a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Combating Label Noise in Deep Learning using Abstention},
	url = {https://proceedings.mlr.press/v97/thulasidasan19a.html},
	volume = {97},
	year = {2019},
	bdsk-url-1 = {https://proceedings.mlr.press/v97/thulasidasan19a.html}}

@article{case-level-context,
  title={Using case-level context to classify cancer pathology reports},
  author={Gao, Shang and Alawad, Mohammed and Schaefferkoetter, Noah and Penberthy, Lynne and Wu, Xiao-Cheng and Durbin, Eric B and Coyle, Linda and Ramanathan, Arvind and Tourassi, Georgia},
  journal={PLoS One},
  volume={15},
  number={5},
  pages={e0232840},
  year={2020},
  doi={10.1371/journal.pone.0232840},
  publisher={Public Library of Science San Francisco, CA USA}
}


@article{alawad2020automatic,
	author = {Alawad, Mohammed and Gao, Shang and Qiu, John X and Yoon, Hong Jun and Blair Christian, J and Penberthy, Lynne and Mumphrey, Brent and Wu, Xiao-Cheng and Coyle, Linda and Tourassi, Georgia},
	date-added = {2022-12-15 15:17:50 -0500},
	date-modified = {2022-12-15 15:17:50 -0500},
	journal = {Journal of the American Medical Informatics Association},
	number = {1},
	pages = {89--98},
	publisher = {Oxford University Press},
	title = {Automatic extraction of cancer registry reportable information from free-text pathology reports using multitask convolutional neural networks},
	volume = {27},
	doi={10.1093/jamia/ocz153},
	year = {2020}}

@article{GAO2019101726,
	abstract = {We introduce a deep learning architecture, hierarchical self-attention networks (HiSANs), designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy, faster training, and clear interpretability. We evaluate performance on a corpus of 374,899 pathology reports obtained from the National Cancer Institute's (NCI) Surveillance, Epidemiology, and End Results (SEER) program. Each pathology report is associated with five clinical classification tasks -- site, laterality, behavior, histology, and grade. We compare the performance of the HiSAN against other machine learning and deep learning approaches commonly used on medical text data -- Naive Bayes, logistic regression, convolutional neural networks, and hierarchical attention networks (the previous state-of-the-art). We show that HiSANs are superior to other machine learning and deep learning text classifiers in both accuracy and macro F-score across all five classification tasks. Compared to the previous state-of-the-art, hierarchical attention networks, HiSANs not only are an order of magnitude faster to train, but also achieve about 1% better relative accuracy and 5% better relative macro F-score.},
	author = {Shang Gao and John X. Qiu and Mohammed Alawad and Jacob D. Hinkle and Noah Schaefferkoetter and Hong-Jun Yoon and Blair Christian and Paul A. Fearn and Lynne Penberthy and Xiao-Cheng Wu and Linda Coyle and Georgia Tourassi and Arvind Ramanathan},
	date-added = {2022-12-15 15:17:39 -0500},
	date-modified = {2022-12-15 15:17:39 -0500},
	doi = {10.1016/j.artmed.2019.101726},
	issn = {0933-3657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Cancer pathology reports, Clinical reports, Deep learning, Natural language processing, Text classification},
	pages = {101726},
	title = {Classifying cancer pathology reports with hierarchical self-attention networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365719303562},
	volume = {101},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0933365719303562},
	bdsk-url-2 = {https://doi.org/10.1016/j.artmed.2019.101726}}

@incollection{pytorch,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	date-modified = {2022-12-15 15:16:16 -0500},
	pages = {8024--8035},
	publisher = {Curran Associates, Inc.},
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	year = {2019},
	bdsk-url-1 = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}


@article{zhao2021pyhealth,
  title={Pyhealth: A python library for health predictive models},
  author={Zhao, Yue and Qiao, Zhi and Xiao, Cao and Glass, Lucas and Sun, Jimeng},
  journal={arXiv preprint arXiv:2101.04209},
  doi = {10.48550/arXiv.2101.04209},
  year={2021}
}

@article{cardoso2022monai,
  title={MONAI: An open-source framework for deep learning in healthcare},
  author={Cardoso, M Jorge and Li, Wenqi and Brown, Richard and Ma, Nic and Kerfoot, Eric and Wang, Yiheng and Murrey, Benjamin and Myronenko, Andriy and Zhao, Can and Yang, Dong and others},
  journal={arXiv preprint arXiv:2211.02701},
  doi={10.48550/arXiv.2211.02701},
  year={2022}
}

@article{golts2023fusemedml,
  title={FuseMedML: a framework for accelerated discovery in machine learning based biomedicine},
  author={Golts, Alex and Raboh, Moshe and Shoshan, Yoel and Polaczek, Sagi and Rabinovici-Cohen, Simona and Hexter, Efrat},
  journal={Journal of Open Source Software},
  volume={8},
  number={81},
  pages={4943},
  year={2023},
  doi={10.21105/joss.04943}
}


@article{kormilitzin2021med7,
  title={Med7: A transferable clinical natural language processing model for electronic health records},
  author={Kormilitzin, Andrey and Vaci, Nemanja and Liu, Qiang and Nevado-Holgado, Alejo},
  journal={Artificial Intelligence in Medicine},
  volume={118},
  pages={102086},
  year={2021},
  doi={10.1016/j.artmed.2021.102086},
  publisher={Elsevier}
}

@article{li2022ehrkit,
  title={Ehrkit: A python natural language processing toolkit for electronic health record texts},
  author={Li, Irene and You, Keen and Tang, Xiangru and Qiao, Yujie and Huang, Lucas and Hsieh, Chia-Chun and Rosand, Benjamin and Radev, Dragomir},
  journal={arXiv preprint arXiv:2204.06604},
  doi={10.48550/arXiv.2204.06604},
  year={2022}
}

@article{harris2022clinical,
  title={Clinical deployment environments: Five pillars of translational machine learning for health},
  author={Harris, Steve and Bonnici, Tim and Keen, Thomas and Lilaonitkul, Watjana and White, Mark J and Swanepoel, Nel},
  journal={Frontiers in Digital Health},
  volume={4},
  doi={10.3389/fdgth.2022.939292},
  year={2022}
}

@misc{Candle,
  author = {National Cancer Institute},
  title = {ECP-Candle},
  year = {2023},
  publisher = {GitHub},
  journal = {CANDLE Exascale Computing Program Application},
  howpublished = {\url{https://github.com/ECP-CANDLE}},
}
